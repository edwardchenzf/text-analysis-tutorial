{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This tutorials just give some examples how to fetch and handle different types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/sample-text-files/sample-text-file-1000kb.txt contains 1656 documents.\n",
      "\n",
      "This is the last document:\n",
      "Nunc eget elit elit. Nulla ornare, orci non maximus gravida, quam mi mollis nunc, eu gravida dui diam at nibh. Donec vitae nibh libero. Donec in eleifend neque. In tellus sapien, eleifend in augue et, pellentesque dictum lacus. Etiam congue porttitor sapien eget egestas. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Fusce feugiat, ipsum ut aliquet fringilla, nibh metus ornare dolor, ut semper urna tellus nec orci. Vivamus eget urna ut nisi dapibus sollicitudin. Aliquam ligula ex, placerat nec nisi convallis, mattis suscipit nulla. Donec cursus nec sem consequat tempus. Aenean et ornare dolor, vel bibendum magna. Sed tempor tincidunt lorem.\n"
     ]
    }
   ],
   "source": [
    "sample_file_name = 'data/sample-text-files/sample-text-file-1000kb.txt'\n",
    "\n",
    "documents = []\n",
    "with open(sample_file_name) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line != '': # Ingnore empty lines\n",
    "            documents.append(line)\n",
    "            \n",
    "print(\"The file {} contains {} documents.\".format(sample_file_name, len(documents)))\n",
    "print()\n",
    "print(\"This is the last document:\")\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV/TSV files\n",
    "\n",
    "In principle, CSV/TSV (comma-separated/tab-separated values) files are also just text files. As such, one can sue the approache from above to read such files. The structured nature of CSV/TSV files quickly leads to annoying issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6e943eae215d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mreview_nr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Oh, oh...can you spot the problem?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This will most likely throw an error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "reviews_file_name = 'data/reviews/yelp-reviews-mon-ami-gabi.csv'\n",
    "\n",
    "with open(reviews_file_name) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0: # We want to ignore the header\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            review_nr, review_text = line.split(',') # Oh, oh...can you spot the problem?\n",
    "            print(review_text) # This will most likely throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since handling CSV/TSV files is a very common task, there is already powerful Python packages available that makes life some much easier. `pandas` is a very popular package for handling structured files like CSV/TSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` uses the notion of data frames (df) to denote data objects\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent food, great atmosphere, a bit noisy....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>If you enjoy a little people watching with you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>affordable, fairly classic french foodsit outs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Though heartbroken and a bit aimless on my 22n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The food and wine was amazing, but the super h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Yippy!  Make-your-own bloody mary bar! Chose t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>I went here for a team function on one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>An affordable dining experience in Paris, I me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Definitely one of my favorites on the Strip! I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>What great friends I have..... We ate at Mon A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number                                             review\n",
       "0              1  Excellent food, great atmosphere, a bit noisy....\n",
       "1              2  If you enjoy a little people watching with you...\n",
       "2              3  affordable, fairly classic french foodsit outs...\n",
       "3              4  Though heartbroken and a bit aimless on my 22n...\n",
       "4              5  The food and wine was amazing, but the super h...\n",
       "5              6  Yippy!  Make-your-own bloody mary bar! Chose t...\n",
       "6              7  I went here for a team function on one of the ...\n",
       "7              8  An affordable dining experience in Paris, I me...\n",
       "8              9  Definitely one of my favorites on the Strip! I...\n",
       "9             10  What great friends I have..... We ate at Mon A..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(reviews_file_name, sep=',', quotechar='\"', encoding = \"ISO-8859-1\")\n",
    "df = pd.read_csv(reviews_file_name, encoding = \"ISO-8859-1\")\n",
    "\n",
    "df.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/reviews/yelp-reviews-mon-ami-gabi.csv contains 1000 reviews.\n",
      "\n",
      "This is the last review:\n",
      "I had a $25 lettuce entertain you gift card so I brought it with me to Las Vegas and had a very pricey breakfast, but ... no complaints, it was good.We got seated right away outside, it was still reasonably comfortable enough to eat outside in July in Las Vegas.  We were told the specials, I opted for one of them - a fancy eggs benedict and apple juice, which was a lighter color as it was REAL apple juice, not the kind from a machine or bottle.  We also started our meal with a blackberry bran muffin which was super yummy.\n"
     ]
    }
   ],
   "source": [
    "# Extract list of reviews from data frame\n",
    "reviews = df['review'].tolist()\n",
    "\n",
    "print(\"The file {} contains {} reviews.\".format(reviews_file_name, len(reviews)))\n",
    "print()\n",
    "print(\"This is the last review:\")\n",
    "print(reviews[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online news article\n",
    "\n",
    "This example addresses online content. Handling \"raw\" websites is usually a bit annoying since the text is not plain text but HTML. For simplicity, we use the package `newspaper` that helps to fetch the content of online news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to copy&paste different news article URLs. Note the package does not work with all news websites; however, it works just fine with straitstimes.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears'\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods `download()` and `parse()` do the actually fetching and processing of the news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: [] \n",
      "\n",
      "Publication data: 2017-10-19 22:15:56+08:00 \n",
      "\n",
      "Title: Now its Japan's turn to brace for a monster storm as Typhoon Lan nears \n",
      "\n",
      "Main text: NEW YORK (BLOOMBERG) - Seems like no one can escape nature's wrath these days.\n",
      "\n",
      "Typhoon Lan is forecast to grow into a monster storm south of Japan before it weakens on its approach to the island nation next week. It come on the heels of Ophelia, which brought gale-force winds to southern Ireland Monday, Maria, which devastated Puerto Rico, and Irma, Harvey and Nate, which struck the US Gulf Coast or Florida.\n",
      "\n",
      "That's not to mention two hurricanes that recently struck Mexico.\n",
      "\n",
      "Lan's top winds could reach 138 miles (222 kilometers) per hour Saturday, which would make it the equivalent of a Category 4 hurricane on the five-step Saffir-Simpson scale, according to the Joint Typhoon Warning Center in Hawaii. As it nears the Tokyo-Yokohama area, winds will probably weaken to about 109 mph, making it a Category 2 storm.\n",
      "\n",
      "Lan isn't the first big storm for Japan this season. It was struck by typhoons Noru and Talim, in August in September. \n",
      "\n",
      "Top image link: https://static.straitstimes.com.sg/sites/default/files/styles/x_large/public/articles/2017/10/19/fa-lan-20171019.jpg?itok=tIVcW4az \n",
      "\n",
      "Video links: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Authors:\", article.authors, \"\\n\")\n",
    "print(\"Publication data:\", article.publish_date, \"\\n\")\n",
    "print(\"Title:\", article.title, \"\\n\")\n",
    "print(\"Main text:\", article.text, \"\\n\")\n",
    "print(\"Top image link:\", article.top_image, \"\\n\")\n",
    "print(\"Video links:\", article.movies, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `newspaper` packages comes with some additional functions to extract keywords and generate a summary for a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['typhoon', 'brace', 'lan', 'japan', 'struck', 'weaken', 'winds', 'week', 'wrath', 'category', 'weakens', 'monster', 'turn', 'york', 'nears', 'storm', 'japans'] \n",
      "\n",
      "Summary: NEW YORK (BLOOMBERG) - Seems like no one can escape nature's wrath these days.\n",
      "Typhoon Lan is forecast to grow into a monster storm south of Japan before it weakens on its approach to the island nation next week.\n",
      "As it nears the Tokyo-Yokohama area, winds will probably weaken to about 109 mph, making it a Category 2 storm.\n",
      "Lan isn't the first big storm for Japan this season.\n",
      "It was struck by typhoons Noru and Talim, in August in September. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "article.nlp()\n",
    "print(\"Keywords:\", article.keywords, \"\\n\")\n",
    "print(\"Summary:\", article.summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter provides an API (Application Programming Interface) that allows to fetch public tweets. The `twython` packages is a wrapper for ths API to simplify this task in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twython import Twython, TwythonError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the API requires credentials. This in turn requires a Twitter account and further configurations. If you don't have or want an Twitter account then no problem. This tutorial is only supposed to show how simply the task of fetching tweets is. It won't be required for the other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APP_KEY = '' \n",
    "APP_SECRET = '' \n",
    "OAUTH_TOKEN = '' \n",
    "OAUTH_TOKEN_SECRET = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other calls, the Twitter API allows to search for tweets using keywords. You can also specify that you're not interest in retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter API returned a 400 (Bad Request), Bad Authentication data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search_results = twitter.search(q='\"orchard road\" -filter:retweets', count=20)\n",
    "except TwythonError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet comes with a plethora of information. In the following we are only interest in the user name, the date and time the tweet was posted and the tweet text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cc592d12ad79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'statuses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[1;31m# Ingnore non-English tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lang'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_results' is not defined"
     ]
    }
   ],
   "source": [
    "for tweet in search_results['statuses']:\n",
    "    # Ingnore non-English tweets\n",
    "    language = tweet['lang']\n",
    "    if language != 'en':\n",
    "        continue\n",
    "    # Extract the basic information about the tweet\n",
    "    screen_name = tweet['user']['screen_name']\n",
    "    created_at =  tweet['created_at']\n",
    "    tweet_text = tweet['text']    \n",
    "    # Simple way to remove line breaks and tabs: string to list and back to string again\n",
    "    tweet_text = ' '.join(tweet_text.split())\n",
    "    # Twitter returns the time as string of the form \"Wed Jan 24 10:37:57 +0000 2018\"; let's simplify this\n",
    "    created_at = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(created_at,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "    # Print each tweet with publication date, the screen name of the user, and the actual text of the tweet\n",
    "    print('[{}] @{} wrote: {}'.format(created_at, screen_name, tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
