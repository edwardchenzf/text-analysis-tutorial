{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (Deep Learning, LSTM)\n",
    "\n",
    "Named Entity Recognition (NER) is a sequence labelling task: Given a sequence of word, we want to assign each word with a label which indicates whether the word is or is part of a named entity. There are different machine learning approaches to solve sequence labelling, including Hidden Markov Chains, Conditional Random Fields, etc. Using deep learning, the recurrent network model can be sketched as follows:\n",
    "\n",
    "<img src=\"images/rnn-many-to-many-sequence-labelling.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "In contrast to traditional sentiment analysis, we no longer just assign one label/class to a sentences but one label/class to each word in the sentence. As you will see in the following, the most effort will be in the data preparation to get a dataset into the shape to serve as input and output for the deep learning network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Activation, Embedding, Dense, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset file\n",
    "\n",
    "If you look into the text file `data/ner-dataset/wikigold.conll.txt` you can see that each line contains a word-label pair seperated by a whitespace. Consecutive pairs form a sentence, and sentences are separated by an empty line. In the following loop, we read the text file line by line, connect word-label pairs so sentences and organize all sentences in to a list `sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xa0 in position 4437: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd7bb25a7c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/ner-dataset/wikigold.conll.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mstripped_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;31m#stripped_line = line.strip().lower().split(' ') # You can use this line instead to see the effect to make all words lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xa0 in position 4437: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "sentence = []\n",
    "\n",
    "with open('data/ner-dataset/wikigold.conll.txt') as f:\n",
    "    for line in f:\n",
    "        stripped_line = line.strip().split(' ')\n",
    "        #stripped_line = line.strip().lower().split(' ') # You can use this line instead to see the effect to make all words lowercase\n",
    "        \n",
    "        if len(stripped_line) == 2: # Add only valid word-label pairs to sentence list\n",
    "            sentence.append(stripped_line)\n",
    "        \n",
    "        if line == '\\n': # In this case, we have reached the end of the current sentence\n",
    "            sequences.append(sentence) # Add current sentence to overall list\n",
    "            sentence = []              # Clear current sentence list\n",
    "        \n",
    "print('Number of sequences: {}'.format(len(sequences)))\n",
    "print()\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove overly long sequences\n",
    "\n",
    "There is the intrinsic goal to keep the maximum length of sequences small since this number represents the number of time steps for the recurrent neural network (here: LSTM) which in turn heavily affects the training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [ len(x) for x in sequences ] \n",
    "max_seq_len = max(sequence_lengths)\n",
    "\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the distrubtion of sequence lengths using a histogram. There we can check what might be a good cut-off threshold that (a) minimizes the maximum sequence length but (b) still keeps most of the data we need for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFkCAYAAAD7dJuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9feab53eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(sequence_lengths, normed=True, bins=30)\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on this graph, we can easily kick out all sequences longer than 64 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of short sequences: 360\n"
     ]
    }
   ],
   "source": [
    "short_sequences = [ s for s in sequences if len(s) <= 64 ]\n",
    "\n",
    "print('Number of short sequences: {}'.format(len(short_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make a sanity check, let's calculate the longest sequence again. The results shouldn't be surprising..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max([ len(x) for x in short_sequences ])\n",
    "\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in the input sequences and label sequences\n",
    "\n",
    "`short_sequences` is a list which itself contains lists (i.e., each sentence) of lists (i.e., each word-label pair). We need to split th data in to `X` as the list of sentences and `y` as the list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[c[0] for c in x] for x in short_sequences]\n",
    "y = [[c[1] for c in y] for y in short_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example, let's see the words and respective labels of the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['010', 'is', 'the', 'tenth', 'album', 'from', 'Japanese', 'Punk', 'Techno', 'band', 'The', 'Mad', 'Capsule', 'Markets', '.']\n",
      "['I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vocabulary and list of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract all words and all labels from all sequences. Note that these list contain the same word or label multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_word_list = [c for x in X for c in x]\n",
    "full_label_list = [c for x in y for c in x] # A bit overkill since we know the 5 available labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `set()` constructor on both lists is an easy way to convert the lists into sets, which means that duplicates will automatically be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 2395\n",
      "\n",
      "Number of labels: 5\n",
      "['I-LOC', 'O', 'I-ORG', 'I-MISC', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(full_word_list))\n",
    "labels =  list(set(full_label_list))\n",
    "\n",
    "print('Size of vocabulary: {}'.format(len(vocabulary)))\n",
    "print()\n",
    "print('Number of labels: {}'.format(len(labels)))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mappings between words/indexes and labels/indexes\n",
    "\n",
    "Since the network take words but integers as input and output we have to define mappings that map words and labels to unique indexes (i.e., integer values), and vice versa. \n",
    "\n",
    "**Important:** We have to reserve word index 0 to a special word, e.g., \"<PAD>\". Below, we again need to pad short sequences. Since the padding is done with 0's, we have to ensure that 0 is not associated with proper word from the dictionary.\n",
    "\n",
    "In the tutorial for sentiment analysis using deep learning, we could use the tokenizer provided by Keras which conveniently created the require mapping(s). Here it is actually easier to to it \"manually\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4978",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7e372a0e6aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0midx_to_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'the'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4978\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'I-LOC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4978"
     ]
    }
   ],
   "source": [
    "word_to_idx = {word: (idx + 2) for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {(idx + 2): word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Explicitly set the index of the \"<PAD>\" token\n",
    "word_to_idx['<PAD>'] = 0\n",
    "idx_to_word[0] = '<PAD>'\n",
    "\n",
    "word_to_idx['<UNK>'] = 1\n",
    "idx_to_word[1] = '<UNK>'\n",
    "\n",
    "\n",
    "label_to_idx = {label: (idx + 1) for idx, label in enumerate(labels)}\n",
    "idx_to_label = {(idx + 1): label for idx, label in enumerate(labels)}\n",
    "\n",
    "print(word_to_idx['the'], idx_to_word[4978])\n",
    "print(label_to_idx['I-LOC'], idx_to_label[3])\n",
    "print()\n",
    "print(word_to_idx['<PAD>'], idx_to_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode word and label sequences\n",
    "\n",
    "With the mappings we can now encode the words sequences and pad them so they all have the same length `max_seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['010', 'is', 'the', 'tenth', 'album', 'from', 'Japanese', 'Punk', 'Techno', 'band', 'The', 'Mad', 'Capsule', 'Markets', '.']\n",
      "[ 870 1217  481 1692 1161  742 1297 1153 1728  594   65  987  596  392\n",
      " 1749    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = [[word_to_idx[word] for word in sentence] for sentence in X]\n",
    "X_encoded = pad_sequences(X_encoded, maxlen=max_seq_len, padding='post')\n",
    "\n",
    "print(X[0])\n",
    "print(X_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the example above that \"the\" and \"The\" are considered different words. For NER it is often beneficial not to lowercase all words since named entities are often capitalized. In formal text, this assumption typical holds. In informal text like social media, people are typically less careful when it comes to proper capitalization.\n",
    "\n",
    "Encoding the labels is a bit more tricky, since each label needs to be encoded as a one-hot vector instead of a single integer value -- the word indexs will eventually also be converted into word vectors, but this will be done by the `Embedding` layer in the network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 2 2 2 2 4 2 2 2 3 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "[array([0., 0., 0., 0., 1., 0.]), array([0., 0., 1., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0.])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utility function that returns an array of size n, where all elements are zero except at position x, where it is 1.\n",
    "# (i.e., it creates a one-hot encoding for an individual label)\n",
    "def encode(x, n):\n",
    "    result = np.zeros(n)\n",
    "    result[x] = 1\n",
    "    return result\n",
    "\n",
    "# Same as for X_encoded: map label to index and pad to length max_seq_length              \n",
    "y_encoded = [ [label_to_idx[label] for label in label_seq] for label_seq in y ]\n",
    "y_encoded = pad_sequences(y_encoded, maxlen=max_seq_len, padding='post')\n",
    "print(y_encoded[0])\n",
    "print()\n",
    "\n",
    "# Convert each label index to the corresponding one-hot index\n",
    "y_encoded = [ [encode(label, len(labels)+1) for label in label_seq] for label_seq in y_encoded]\n",
    "print(y_encoded[0][:3])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 1, 'O': 2, 'I-ORG': 3, 'I-MISC': 4, 'I-PER': 5}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_encoded = np.array(X_encoded)\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test data\n",
    "\n",
    "`scitkit-learn` provides a convenient method to split the data into training and test data given a user-defined ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 324\n",
      "Size of test set: 36\n",
      "\n",
      "(324, 64)\n",
      "[1195  667 2118 1707  481 1290  706 2144 1053  481 1674 1051 1418  180\n",
      "  174  841  481  983 1876 1587 1749    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.1)\n",
    "\n",
    "print('Size of training set: {}'.format(len(X_train)))\n",
    "print('Size of test set: {}'.format(len(X_test)))\n",
    "\n",
    "print()\n",
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We have to specify a set of parameters for the network. You can change those value to see the effects on the accuracy and performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_DIM = 32\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "max_features = len(vocabulary) + 2 # +1 because we added the word '<PAD>'; +1 because we added the word '<UNK>'\n",
    "out_size = len(labels) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We define a common network structure containing\n",
    "\n",
    "- an embedding layer\n",
    "- a bi-directional LSTM layer\n",
    "- a fully connected (dense) layer as output for **each time step** (hence `TimeDistributed`)\n",
    "- a softmax layer to normalize the output to probabilities (again, for each time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 128)           306816    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64, 64)            41216     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 64, 6)             390       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 6)             0         \n",
      "=================================================================\n",
      "Total params: 348,422\n",
      "Trainable params: 348,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, EMBEDDING_DIM, input_length=max_seq_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(LSTM_HIDDEN_DIM, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(out_size)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 291 samples, validate on 33 samples\n",
      "Epoch 1/100\n",
      "291/291 [==============================] - 4s 12ms/step - loss: 1.7379 - val_loss: 1.6155\n",
      "Epoch 2/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 1.4899 - val_loss: 1.1512\n",
      "Epoch 3/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.9475 - val_loss: 0.6357\n",
      "Epoch 4/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.8290 - val_loss: 0.6131\n",
      "Epoch 5/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.7323 - val_loss: 0.6057\n",
      "Epoch 6/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.6859 - val_loss: 0.5824\n",
      "Epoch 7/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.6442 - val_loss: 0.5400\n",
      "Epoch 8/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.6008 - val_loss: 0.5138\n",
      "Epoch 9/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.5538 - val_loss: 0.4857\n",
      "Epoch 10/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.5072 - val_loss: 0.4590\n",
      "Epoch 11/100\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.4708 - val_loss: 0.4345\n",
      "Epoch 12/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.4330 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.3880 - val_loss: 0.3979\n",
      "Epoch 14/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.3579 - val_loss: 0.3813\n",
      "Epoch 15/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.3239 - val_loss: 0.3667\n",
      "Epoch 16/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.2930 - val_loss: 0.3558\n",
      "Epoch 17/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.2604 - val_loss: 0.3441\n",
      "Epoch 18/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.2316 - val_loss: 0.3327\n",
      "Epoch 19/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.2068 - val_loss: 0.3224\n",
      "Epoch 20/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.1789 - val_loss: 0.3118\n",
      "Epoch 21/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.1517 - val_loss: 0.3085\n",
      "Epoch 22/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.2977\n",
      "Epoch 23/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.1114 - val_loss: 0.3028\n",
      "Epoch 24/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0935 - val_loss: 0.2873\n",
      "Epoch 25/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0797 - val_loss: 0.3009\n",
      "Epoch 26/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0685 - val_loss: 0.2967\n",
      "Epoch 27/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0589 - val_loss: 0.2941\n",
      "Epoch 28/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0516 - val_loss: 0.3019\n",
      "Epoch 29/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0447 - val_loss: 0.3019\n",
      "Epoch 30/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0406 - val_loss: 0.3079\n",
      "Epoch 31/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0355 - val_loss: 0.3062\n",
      "Epoch 32/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0322 - val_loss: 0.3092\n",
      "Epoch 33/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0286 - val_loss: 0.3133\n",
      "Epoch 34/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0265 - val_loss: 0.3122\n",
      "Epoch 35/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0242 - val_loss: 0.3223\n",
      "Epoch 36/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0222 - val_loss: 0.3127\n",
      "Epoch 37/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0206 - val_loss: 0.3236\n",
      "Epoch 38/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0190 - val_loss: 0.3172\n",
      "Epoch 39/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0176 - val_loss: 0.3244\n",
      "Epoch 40/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0166 - val_loss: 0.3220\n",
      "Epoch 41/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0156 - val_loss: 0.3243\n",
      "Epoch 42/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0144 - val_loss: 0.3271\n",
      "Epoch 43/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0136 - val_loss: 0.3268\n",
      "Epoch 44/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0125 - val_loss: 0.3253\n",
      "Epoch 45/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0118 - val_loss: 0.3280\n",
      "Epoch 46/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0111 - val_loss: 0.3291\n",
      "Epoch 47/100\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.0105 - val_loss: 0.3305\n",
      "Epoch 48/100\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.0100 - val_loss: 0.3308\n",
      "Epoch 49/100\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.0095 - val_loss: 0.3333\n",
      "Epoch 50/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0090 - val_loss: 0.3347\n",
      "Epoch 51/100\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.0085 - val_loss: 0.3345\n",
      "Epoch 52/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0081 - val_loss: 0.3334\n",
      "Epoch 53/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.3341\n",
      "Epoch 54/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.3357\n",
      "Epoch 55/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.3373\n",
      "Epoch 56/100\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.0067 - val_loss: 0.3379\n",
      "Epoch 57/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0064 - val_loss: 0.3391\n",
      "Epoch 58/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0060 - val_loss: 0.3403\n",
      "Epoch 59/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0060 - val_loss: 0.3412\n",
      "Epoch 60/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0057 - val_loss: 0.3416\n",
      "Epoch 61/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.3437\n",
      "Epoch 62/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.3445\n",
      "Epoch 63/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.3443\n",
      "Epoch 64/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.3475\n",
      "Epoch 65/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.3476\n",
      "Epoch 66/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.3451\n",
      "Epoch 67/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.3468\n",
      "Epoch 68/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.3472\n",
      "Epoch 69/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.3505\n",
      "Epoch 70/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0038 - val_loss: 0.3522\n",
      "Epoch 71/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.3487\n",
      "Epoch 72/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.3489\n",
      "Epoch 73/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.3495\n",
      "Epoch 74/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.3510\n",
      "Epoch 75/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0032 - val_loss: 0.3534\n",
      "Epoch 76/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0031 - val_loss: 0.3556\n",
      "Epoch 77/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0030 - val_loss: 0.3546\n",
      "Epoch 78/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0029 - val_loss: 0.3555\n",
      "Epoch 79/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.3568\n",
      "Epoch 80/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.3570\n",
      "Epoch 81/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.3572\n",
      "Epoch 82/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0026 - val_loss: 0.3591\n",
      "Epoch 83/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0025 - val_loss: 0.3595\n",
      "Epoch 84/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0024 - val_loss: 0.3593\n",
      "Epoch 85/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0023 - val_loss: 0.3597\n",
      "Epoch 86/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0023 - val_loss: 0.3606\n",
      "Epoch 87/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.3610\n",
      "Epoch 88/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.3606\n",
      "Epoch 89/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.3622\n",
      "Epoch 90/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.3643\n",
      "Epoch 91/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.3647\n",
      "Epoch 92/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.3660\n",
      "Epoch 93/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.3678\n",
      "Epoch 94/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.3697\n",
      "Epoch 95/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.3675\n",
      "Epoch 96/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.3681\n",
      "Epoch 97/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.3684\n",
      "Epoch 98/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.3691\n",
      "Epoch 99/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.3688\n",
      "Epoch 100/100\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.3692\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "We can use the in-built method `evaluate()` as usual to calculate the overall accuracy of the model over the test set. However, be aware that we do not simple check of a single class has been predicted currectly. Here, 100% accuracy would now mean that the label for each word in all sequences is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 1ms/step\n",
      "0.45314064290788436\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look pretty good. Note that the evaluate can also be application specific. For example, let's take the following sentence \"I stay in the Hilton Hotel\", and the phrase \"The Hilton Hotel\" is labeled as \"I-LOC\" in our dataset. If the our model only label \"Hilton Hotel\" as a location, it would not be 100% correct. One can arguem, however, that this results is correct (enough).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1292  493 1473 1173 1116 1173 1064 2207   90 1707  481 1170 1998 1053\n",
      "   768 1749    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "Five O O\n",
      "months O O\n",
      "later O O\n",
      ", O O\n",
      "however O O\n",
      ", O O\n",
      "it O O\n",
      "did O O\n",
      "participate O O\n",
      "in O O\n",
      "the O O\n",
      "2nd O I-MISC\n",
      "Battle I-MISC I-MISC\n",
      "of I-MISC I-MISC\n",
      "Fredericksburg I-MISC I-MISC\n",
      ". O O\n"
     ]
    }
   ],
   "source": [
    "# Pick a random position of an item in the test data\n",
    "sample_pos = 10\n",
    "\n",
    "# Get the word and label sequence at that position\n",
    "sample_seq = np.asarray([X_test[sample_pos]])\n",
    "sample_labels = np.asarray(y_test[sample_pos])\n",
    "\n",
    "# Convert to a word sequence and get the label\n",
    "word_list = [ idx_to_word[idx] for idx in sample_seq[0] ]\n",
    "label_list = [ np.argmax(label_seq) for label_seq in sample_labels ]\n",
    "\n",
    "print(sample_seq)\n",
    "\n",
    "# Use the trained model to predict the labels for the sample sequence\n",
    "y_pred = model.predict_classes([sample_seq])\n",
    "\n",
    "# Print the word, the predicted label and the true label for each word\n",
    "for i, word in enumerate(word_list):\n",
    "    if word == '<PAD>':\n",
    "        break # If we reach the end of the \"real\" words, we can stop\n",
    "    print(word, idx_to_label[y_pred[0][i]], idx_to_label[label_list[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application use case: extracting named entities from news articles\n",
    "\n",
    "In a previous tutorial we used NLTK and spaCy to extract named entities from news articles. We can now perform this task using our own trained network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "We first need some more packages to fetch news articles and to handle the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility method\n",
    "\n",
    "The following method takes a `sentence` and extracts all found named entities using a traind `model` and the respective `word_to_idx` and `idx_to_label`. The first part convets the sentence into sequence the model understand and predicts the labels; see above example code. The second part - the loop - essentially goes trough the whole sequence to form the output as a list of named entities (phrase + label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_named_entities(sentence, model, word_to_idx, idx_to_label):\n",
    "    # Tokenize sentence\n",
    "    seq = word_tokenize(sentence)\n",
    "    # Encode words using the word-to-index mapping\n",
    "    seq_encoded = [ word_to_idx[w] if w in word_to_idx else 1 for w in seq ] # Recall: 1 == <UNK>\n",
    "    # Generate final input data by padding the sequence (and truncating if needed!!!)\n",
    "    X = pad_sequences([seq_encoded], maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "    # Use the trained model to predict the labels for the sample sequence\n",
    "    y_pred = model.predict_classes([X], verbose=0)\n",
    "\n",
    "    # Initialize some variables\n",
    "    named_entities, phrase, current_label = [], [], 0\n",
    "    for i, word in enumerate(seq):\n",
    "        if word == '<PAD>':\n",
    "            break # If we reach the end of the \"real\" words, we can stop\n",
    "        # Get predicted label for the current word\n",
    "        label = idx_to_label[y_pred[0][i]]\n",
    "        if label != 'O':\n",
    "            phrase.append(word)\n",
    "        else:\n",
    "            if len(phrase) > 0:\n",
    "                named_entities.append((' '.join(phrase), current_label))\n",
    "            phrase = []\n",
    "        # Update current label\n",
    "        current_label = label\n",
    "        \n",
    "    # Don't forget named entities at the very end even if there is not punctuation mark\n",
    "    if len(phrase) > 0:\n",
    "        named_entities.append((' '.join(phrase), current_label))\n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example regarding the usage of `extract_named_entities`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hotel', 'I-ORG')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tarzan likes to stay at The Hilton Hotel\"\n",
    "\n",
    "named_entities = extract_named_entities(sentence, model, word_to_idx, idx_to_label)\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing news articles\n",
    "\n",
    "Again, we use the `newspaper` package to easily fetch an online news article and extract the main content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears'\n",
    "#url = 'http://www.straitstimes.com/singapore/ammonia-leak-in-food-factory-at-fishery-port-road-3-taken-to-hospital'\n",
    "#url = 'http://www.straitstimes.com/singapore/police-car-mounts-divider-in-accident-in-kampong-bahru-road-no-injuries'\n",
    "\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "title = article.title\n",
    "text = article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained our model of sequences of a maximum length of 64 words. We therefore cannot / should not give the whole content to the model but do it sentence by sentence. To split the content into sentences, we can simply use the `sent_tokenize()` method of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can loop over each sentence and extract all named entities for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW YORK (BLOOMBERG) - Seems like no one can escape nature's wrath these days.\n",
      "[('nature', 'I-MISC')]\n",
      "\n",
      "\n",
      "Typhoon Lan is forecast to grow into a monster storm south of Japan before it weakens on its approach to the island nation next week.\n",
      "[]\n",
      "\n",
      "\n",
      "It come on the heels of Ophelia, which brought gale-force winds to southern Ireland Monday, Maria, which devastated Puerto Rico, and Irma, Harvey and Nate, which struck the US Gulf Coast or Florida.\n",
      "[('Ireland', 'I-PER'), ('Harvey', 'I-PER')]\n",
      "\n",
      "\n",
      "That's not to mention two hurricanes that recently struck Mexico.\n",
      "[]\n",
      "\n",
      "\n",
      "Lan's top winds could reach 138 miles (222 kilometers) per hour Saturday, which would make it the equivalent of a Category 4 hurricane on the five-step Saffir-Simpson scale, according to the Joint Typhoon Warning Center in Hawaii.\n",
      "[('Warning', 'I-PER')]\n",
      "\n",
      "\n",
      "As it nears the Tokyo-Yokohama area, winds will probably weaken to about 109 mph, making it a Category 2 storm.\n",
      "[]\n",
      "\n",
      "\n",
      "Lan isn't the first big storm for Japan this season.\n",
      "[]\n",
      "\n",
      "\n",
      "It was struck by typhoons Noru and Talim, in August in September.\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    named_entities = named_entities = extract_named_entities(sent, model, word_to_idx, idx_to_label)\n",
    "    print('{}\\n{}\\n\\n'.format(sent, named_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results\n",
    "\n",
    "The results are not bad at all but (of course) far from perfect. There are a couple of reasons for that:\n",
    "\n",
    "- We trained over a very small dataset. 1,500 sentences are arbitrary unlikely to contain enough information and a variety to contain all common language structures that indicate a named entity.\n",
    "- With the small dataset comes also a small vocabulary. As such, the will model often have to deal with unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
