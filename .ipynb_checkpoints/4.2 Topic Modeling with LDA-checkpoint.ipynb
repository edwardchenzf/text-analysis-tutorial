{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# The next imports are only needed for the preprocessing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from utils.nlputil import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintion of toy dataset\n",
    "\n",
    "For this simple example, we define our corpus as a list of documents. Each documents is only a single sentence to keep the example easy to follow. Naturally, a document may contain a large number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"cats and dogs are both domesticated animals.\",\n",
    "             \"the domestication of dogs started 10,000 years ago.\",\n",
    "             \"dogs were easier to domensticate than cats.\",\n",
    "             \"Some people have a dog and a cat (or several dogs and cat) as pets.\",\n",
    "             \"the domestication of animals was an important part of human progress.\",\n",
    "             \"python is a programming laguage that is easy to learn\",\n",
    "             \"python makes text processing rather easy.\",\n",
    "             \"a lot of programming languages support text analysis.\",\n",
    "             \"programming in python makes the analysis of text easy\",\n",
    "             \"nltk is a great nlp package for python.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocessing\n",
    "\n",
    "Preprocessing is not required by LDA, but is often intuitive to, for example, consider the singular and plural forms of nouns as the same term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess_text()` method sllows for all kinds of parameters to affect the preprocessing; see the file `nlputil.py` for more details. You can try different parameters and see the how the documents but also the results of LDA change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of length len(documents) initialized with empty strings\n",
    "# (we don't want to override the original documents here)\n",
    "processed_documents = [''] * len(documents)\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    #processed_documents[idx] = preprocess_text(doc)\n",
    "    #processed_documents[idx] = preprocess_text(doc, stemmer=porter_stemmer)\n",
    "    processed_documents[idx] = preprocess_text(doc, lemmatizer=wordnet_lemmatizer)\n",
    "\n",
    "# Print the processed documents\n",
    "for doc in processed_documents:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000 # Top 1000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CountVectorizer` is, among other vectorizers, a handy and flexible way to generate a document term matrix. Mores specificall, here each value in the matrix represents the count how of a term $t_i$ occurs in document $d_j$.\n",
    "\n",
    "The `CountVectorizer` class allows for a wide range of useful input parameters to configurate the generation of the document term matrix; In this example, we use the following:\n",
    "\n",
    "- `max_df`: If not `None` one can specify how often a word has to be in the corpus AT MOST, either in relative terms or in absilute terms. This allows to ingnore words that a very COMMON across all documents and that are not very discriminative.\n",
    "- `min_df`: If not `None` one can specify how often a word has to be in the corpus AT LEAST, either in relative terms or in absilute terms. This allows to ingnore rare words that a very RARE across all documents and that are not very discriminative.\n",
    "- `max_features`: If not `None` one can limit the number of words to ones with the highest counts (term frequencies) acroess the whole corpus\n",
    "- `stop_words`: If not `None` one can specify the list of stop words to be removed from each document (not really necessary if stop words are removed during preprocessing)\n",
    "\n",
    "The `CountVectorizer` accepts more optional input parameters, see here:\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_words, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf = tf_vectorizer.fit_transform(processed_documents)\n",
    "vocabulary = tf_vectorizer.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize document term matrix\n",
    "\n",
    "Just for illustrative purposes, let's print the document term matrix. This is only meaningful for the toy datasets, but highlights the the effects of the different preprocessing options evne before performing LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "print(DataFrame(tf.A, columns=vocabulary).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform LDA\n",
    "\n",
    "First, we need to set the number of topics. In practice, this is not know a-priori. For our toy example, we know to expect 2 topics. You can still change the value and then interpret compare the different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=100, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the model are not probabilites, i.e., the values do to sum up to 1. In most cases, this is not a problem since the absolute values but the relative differences are the important parts. In other words, most of the time these values do not matter at all.\n",
    "\n",
    "However, for illustrative purposes, we can normalize all values to proper probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the results\n",
    "\n",
    "### Show distribution of words for topics\n",
    "\n",
    "`display_topics()` is just a utility method to display the results. For each topic, it ranks all word with respect to the probabilities and list the top *N* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_features):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic {}\".format(topic_idx))\n",
    "        for feature_idx in topic.argsort()[:-num_top_features-1:-1]:\n",
    "            print (\"\\t{0:20} {1}\".format(feature_names[feature_idx],topic[feature_idx]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 50\n",
    "display_topics(lda, vocabulary, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show which document belongs to which topic\n",
    "\n",
    "The method `transform()` takes as input a document word matrix X and returns Document topic distribution for X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `display_documents()` shows the topic for each document. To this end, the method picks to topic with the highest probability. Recall that each document is a distribution over all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_documents(document_topic_matrix, max_documents=10):\n",
    "    num_documents = document_topic_matrix.shape[0]    # Get the number of documents\n",
    "    for n in range(min(num_documents,max_documents)): # Never show more than #max_documents documents\n",
    "        topic_distribution = document_topic_matrix[n] # List of probabilities, e.g., [0.032, 0.233, 0.001, ...]\n",
    "        topic_most_pr = topic_distribution.argmax()   # Pick the list index with the highest probability\n",
    "        print(\"doc: {}   topic: {}\".format(n,topic_most_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the results for the toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_documents(doc_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic assigment should be in line with our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize distribution of words for topics using word clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and plotting word clouds requires some additional imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotutil import show_wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    feature_distribution = lda.components_[topic]\n",
    "    word_frequencies = [ (vocabulary[idx], prob) for idx, prob in enumerate(feature_distribution) ]\n",
    "    # Alternative in case of errors\n",
    "    #for idx, prob in enumerate(feature_distribution):\n",
    "    #    word_frequencies[vocabulary[idx]] = prob\n",
    "    show_wordcloud(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application use case: news article headlines\n",
    "\n",
    "In this example, we apply LDA over a list of 12,394 news article headlines from TechCrunch (https://techcrunch.com/). This dataset is publicly available on Kaggle (https://www.kaggle.com/), see the full link here:\n",
    "- https://www.kaggle.com/PromptCloudHQ/titles-by-techcrunch-and-venturebeat-in-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load news article headlines from CSV file\n",
    "\n",
    "`pandas` is a very popular package for handling structured files like CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/news-articles/news-article-headlines-techcrunch.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Remove rows where Title is \"NaN\" to avoid any errors later on\n",
    "df = df[pd.notnull(df['title'])]\n",
    "\n",
    "# Extract list of headline from data frame\n",
    "news_headlines = df['title'].tolist()\n",
    "\n",
    "# Print the first 5 headlines\n",
    "for idx in range(5):\n",
    "    print (news_headlines[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all news article headlines\n",
    "\n",
    "Store process headlines in a new list to avoid overwriting the orginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news_headlines = [''] * len(news_headlines)\n",
    "\n",
    "for idx, doc in enumerate(news_headlines):\n",
    "    processed_news_headlines[idx] = preprocess_text(doc)\n",
    "    #processed_news_headlines[idx] = preprocess_text(doc, stemmer=porter_stemmer)\n",
    "    #processed_news_headlines[idx] = preprocess_text(doc, lemmatizer=wordnet_lemmatizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000 # Top 1000 words\n",
    "\n",
    "tf_vectorizer_news_headlines = CountVectorizer(max_df=0.95, min_df=1, max_features=num_words, stop_words='english')\n",
    "\n",
    "tf_news_headlines = tf_vectorizer_news_headlines.fit_transform(processed_news_headlines)\n",
    "\n",
    "vocabulary_news_headlines = tf_vectorizer_news_headlines.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform LDA\n",
    "\n",
    "Since these are 12k+ documents, setting the number of topics to 2 is usually not very meaningful. There are not straightforwards rules how to set this number. A common value to start with is 20, inspect the results, and potentially repeat this step with different values.\n",
    "\n",
    "**Note:** This will take now several seconds or even minutes, but still managable. If you have (really) large data, it it is recommended to apply LDA first on a sample to see if all works (no errors) and if the results \"look\" meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "lda_news_headlines = LatentDirichletAllocation(n_topics=num_topics, max_iter=100, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda_news_headlines.fit(tf_news_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    feature_distribution = lda_news_headlines.components_[topic]\n",
    "    word_frequencies = [ (vocabulary_news_headlines[idx], prob) for idx, prob in enumerate(feature_distribution) ]\n",
    "    # Alternative in case of errors\n",
    "    #for idx, prob in enumerate(feature_distribution):\n",
    "    #    word_frequencies[vocabulary[idx]] = prob\n",
    "    show_wordcloud(word_frequencies, max_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_headlines = lda_news_headlines.transform(tf_news_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_documents(doc_topic_headlines, max_documents=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
