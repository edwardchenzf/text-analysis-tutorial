{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# The next imports are only needed for the preprocessing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from utils.nlputil import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintion of toy dataset\n",
    "\n",
    "For this simple example, we define our corpus as a list of documents. Each documents is only a single sentence to keep the example easy to follow. Naturally, a document may contain a large number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"cats and dogs are both domesticated animals.\",\n",
    "             \"the domestication of dogs started 10,000 years ago.\",\n",
    "             \"dogs were easier to domensticate than cats.\",\n",
    "             \"Some people have a dog and a cat (or several dogs and cat) as pets.\",\n",
    "             \"the domestication of animals was an important part of human progress.\",\n",
    "             \"python is a programming laguage that is easy to learn\",\n",
    "             \"python makes text processing rather easy.\",\n",
    "             \"a lot of programming languages support text analysis.\",\n",
    "             \"programming in python makes the analysis of text easy\",\n",
    "             \"nltk is a great nlp package for python.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocessing\n",
    "\n",
    "Preprocessing is not required by LDA, but is often intuitive to, for example, consider the singular and plural forms of nouns as the same term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess_text()` method sllows for all kinds of parameters to affect the preprocessing; see the file `nlputil.py` for more details. You can try different parameters and see the how the documents but also the results of the clustering change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of length len(documents) initialized with empty strings\n",
    "# (we don't want to override the original documents here)\n",
    "processed_documents = [''] * len(documents)\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    #processed_documents[idx] = preprocess_text(doc)\n",
    "    #processed_documents[idx] = preprocess_text(doc, stemmer=porter_stemmer)\n",
    "    processed_documents[idx] = preprocess_text(doc, lemmatizer=wordnet_lemmatizer)\n",
    "\n",
    "# Print the processed documents\n",
    "for doc in processed_documents:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000 # Top 1000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` is, among other vectorizers, a handy and flexible way to generate a document term matrix. Mores specifically, here each value in the matrix represents the `tf-idf` value of a term $t_i$ occurs in document $d_j$. The `tf-idf` value of a term $t_i$ in a document $d_j$ is defined as:\n",
    "\n",
    "$$tfidf(t_i, d_j) = tf(t_i, d_j) \\cdot idf(t_i, d_j)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$ tf(t_i, d_j) = \\frac{number\\ of\\ times\\ t_i\\ appears\\ in\\ d_j}{total\\ number\\ of\\ terms\\ in\\ d_j} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ idf(t_i, d_j) = \\log{\\frac{total\\ number\\ of\\ documents}{number\\ of\\ terms\\ containing\\ t_i}} $$\n",
    "\n",
    "The `TfidfVectorizer` class allows for a wide range of useful input parameters to configurate the generation of the document term matrix; In this example, we use the following:\n",
    "\n",
    "- `max_df`: If not `None` one can specify how often a word has to be in the corpus AT MOST, either in relative terms or in absilute terms. This allows to ingnore words that a very COMMON across all documents and that are not very discriminative.\n",
    "- `min_df`: If not `None` one can specify how often a word has to be in the corpus AT LEAST, either in relative terms or in absilute terms. This allows to ingnore rare words that a very RARE across all documents and that are not very discriminative.\n",
    "- `max_features`: If not `None` one can limit the number of words to ones with the highest counts (term frequencies) acroess the whole corpus\n",
    "- `stop_words`: If not `None` one can specify the list of stop words to be removed from each document (not really necessary if stop words are removed during preprocessing)\n",
    "\n",
    "The `TfidfVectorizer` accepts more optional input parameters, see here:\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=num_words, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = tfidf_vectorizer.fit_transform(processed_documents)\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize document term matrix\n",
    "\n",
    "Just for illustrative purposes, let's print the document term matrix. This is only meaningful for the toy datasets, but highlights the the effects of the different preprocessing options evne before performing the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "print(DataFrame(tfidf_model.A, columns=vocabulary).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clutering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=num_clusters)\n",
    "km_model.fit(tfidf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, label in enumerate(km_model.labels_):\n",
    "    print (idx, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visalization of clusters\n",
    "\n",
    "Visualization of clusters is not straightforwards since each document is represented as a multidimensional vecor of the size of the vocabulary (in this simple example: 10 words). To plot the clustering results in a 2d plot requires methods that reduce the dimensionalities down to 2. We consider two approaches:\n",
    "\n",
    "- Principle Component Analysis (PCA)\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to generate the \"dense\" document term matrix. Usually, the document term matrix is internally stored as a sparse matrix, i.e., only the matrix elements that are not 0 are presented. In practice, most elements in the matrix are 0, so a sparse representation saves a lot of storage and computation overhead. The methods for dimension reduction require a dense matrix with all elements. The method `todense()` accomplishes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_model.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method `show_clusters_high_dim()` that encapsulates dimensionality reduction the plotting of the custering results into a 2d plot. For more details, the method can be found in the file `utils/plotutil.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotutil import show_clusters_high_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clusters_high_dim(km_model, X, method='pca')\n",
    "show_clusters_high_dim(km_model, X, method='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clutering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is another clustering algorithm. In contrast to K-Means, there is no need to specify the number of cluster a-priory. DBSCAN tries to find data points that are more \"densely packed\". The degree of density has to be defined by th user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run DBSCAN over our toy dataset. The parameter `eps` specifies the degree of density. You can change the value and see its effct. In fact, I tried various value to find the cluster that match our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model = DBSCAN(eps=0.6, min_samples=2, metric='cosine', algorithm='brute')\n",
    "dbscan_model.fit(tfidf_model)\n",
    "print (dbscan_model.labels_)\n",
    "\n",
    "labels = dbscan_model.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "dbscan_num_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a good value of k for k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_num_clusters = [2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "\n",
    "for num_clusters in range_num_clusters:\n",
    "    km = KMeans(n_clusters=num_clusters, random_state=10)\n",
    "    km.fit(tfidf_model)\n",
    "    labels = km.labels_\n",
    "    silhouette_avg = silhouette_score(tfidf_model, labels)\n",
    "    print(\"For n_clusters =\", num_clusters, \"The average silhouette_score is :\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application use case: news article headlines\n",
    "\n",
    "In this example, we apply k-Means clustering over a list of 12,394 news article headlines from TechCrunch (https://techcrunch.com/). This dataset is publicly available on Kaggle (https://www.kaggle.com/), see the full link here:\n",
    "\n",
    "- https://www.kaggle.com/PromptCloudHQ/titles-by-techcrunch-and-venturebeat-in-2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load news article headlines from CSV file\n",
    "\n",
    "`pandas` is a very popular package for handling structured files like CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/news-articles/news-article-headlines-techcrunch.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Remove rows where Title is \"NaN\" to avoid any errors later on\n",
    "df = df[pd.notnull(df['title'])]\n",
    "\n",
    "# Extract list of headline from data frame\n",
    "news_headlines = df['title'].tolist()\n",
    "\n",
    "# Print the first 5 headlines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "Again, we normalize each docment by converting all words to lowercase, remove stopwords, and lemmatize each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news_headlines = [''] * len(news_headlines)\n",
    "\n",
    "for idx, doc in enumerate(news_headlines):\n",
    "    #processed_news_headlines[idx] = preprocess_text(doc)\n",
    "    #processed_news_headlines[idx] = preprocess_text(doc, stemmer=porter_stemmer)\n",
    "    processed_news_headlines[idx] = preprocess_text(doc, lemmatizer=wordnet_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate document term matrix\n",
    "\n",
    "Analogously to the steps above, we generate the feature set for the clustering as document term matrix with the tf-idf score being the matrix entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, max_features=num_words, stop_words='english')\n",
    "\n",
    "tfidf_model_news = tfidf_vectorizer.fit_transform(processed_news_headlines)\n",
    "\n",
    "print(tfidf_model_news.shape)\n",
    "\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data, we can now run k-Means with different values for k. For each result, we can then calculate the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for num_clusters in [5,10]:\n",
    "#for num_clusters in [5,10,20,30,40,50]:\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=10)\n",
    "    kmeans.fit(tfidf_model_news)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(tfidf_model_news, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(\"k-Mean for k={} done\".format(num_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we stored all silhouette scores in a list, we can simply plot the corresponding graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(silhouette_scores)\n",
    "plt.ylabel('silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's perform the k-Means with k=20 to visualize again the result using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=20)\n",
    "km_model.fit(tfidf_model_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_model_news.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clusters_high_dim(km_model, X, method='pca')\n",
    "#show_clusters_high_dim(km_model, X, method='tsne')  # Takes much longer to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
