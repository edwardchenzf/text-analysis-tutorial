{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (Deep Learning, LSTM)\n",
    "\n",
    "Named Entity Recognition (NER) is a sequence labelling task: Given a sequence of word, we want to assign each word with a label which indicates whether the word is or is part of a named entity. There are different machine learning approaches to solve sequence labelling, including Hidden Markov Chains, Conditional Random Fields, etc. Using deep learning, the recurrent network model can be sketched as follows:\n",
    "\n",
    "<img src=\"images/rnn-many-to-many-sequence-labelling.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "In contrast to traditional sentiment analysis, we no longer just assign one label/class to a sentences but one label/class to each word in the sentence. As you will see in the following, the most effort will be in the data preparation to get a dataset into the shape to serve as input and output for the deep learning network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Activation, Embedding, Dense, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset file\n",
    "\n",
    "If you look into the text file `data/ner-dataset/wikigold.conll.txt` you can see that each line contains a word-label pair seperated by a whitespace. Consecutive pairs form a sentence, and sentences are separated by an empty line. In the following loop, we read the text file line by line, connect word-label pairs so sentences and organize all sentences in to a list `sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "sentence = []\n",
    "\n",
    "with open('data/ner-dataset/wikigold.conll.txt') as f:\n",
    "    for line in f:\n",
    "        stripped_line = line.strip().split(' ')\n",
    "        #stripped_line = line.strip().lower().split(' ') # You can use this line instead to see the effect to make all words lowercase\n",
    "        \n",
    "        if len(stripped_line) == 2: # Add only valid word-label pairs to sentence list\n",
    "            sentence.append(stripped_line)\n",
    "        \n",
    "        if line == '\\n': # In this case, we have reached the end of the current sentence\n",
    "            sequences.append(sentence) # Add current sentence to overall list\n",
    "            sentence = []              # Clear current sentence list\n",
    "        \n",
    "print('Number of sequences: {}'.format(len(sequences)))\n",
    "print()\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove overly long sequences\n",
    "\n",
    "There is the intrinsic goal to keep the maximum length of sequences small since this number represents the number of time steps for the recurrent neural network (here: LSTM) which in turn heavily affects the training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [ len(x) for x in sequences ] \n",
    "max_seq_len = max(sequence_lengths)\n",
    "\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the distrubtion of sequence lengths using a histogram. There we can check what might be a good cut-off threshold that (a) minimizes the maximum sequence length but (b) still keeps most of the data we need for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(sequence_lengths, normed=True, bins=30)\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on this graph, we can easily kick out all sequences longer than 64 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sequences = [ s for s in sequences if len(s) <= 64 ]\n",
    "\n",
    "print('Number of short sequences: {}'.format(len(short_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make a sanity check, let's calculate the longest sequence again. The results shouldn't be surprising..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([ len(x) for x in short_sequences ])\n",
    "\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in the input sequences and label sequences\n",
    "\n",
    "`short_sequences` is a list which itself contains lists (i.e., each sentence) of lists (i.e., each word-label pair). We need to split th data in to `X` as the list of sentences and `y` as the list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[c[0] for c in x] for x in short_sequences]\n",
    "y = [[c[1] for c in y] for y in short_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example, let's see the words and respective labels of the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vocabulary and list of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract all words and all labels from all sequences. Note that these list contain the same word or label multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_word_list = [c for x in X for c in x]\n",
    "full_label_list = [c for x in y for c in x] # A bit overkill since we know the 5 available labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `set()` constructor on both lists is an easy way to convert the lists into sets, which means that duplicates will automatically be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(full_word_list))\n",
    "labels =  list(set(full_label_list))\n",
    "\n",
    "print('Size of vocabulary: {}'.format(len(vocabulary)))\n",
    "print()\n",
    "print('Number of labels: {}'.format(len(labels)))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mappings between words/indexes and labels/indexes\n",
    "\n",
    "Since the network take words but integers as input and output we have to define mappings that map words and labels to unique indexes (i.e., integer values), and vice versa. \n",
    "\n",
    "**Important:** We have to reserve word index 0 to a special word, e.g., \"<PAD>\". Below, we again need to pad short sequences. Since the padding is done with 0's, we have to ensure that 0 is not associated with proper word from the dictionary.\n",
    "\n",
    "In the tutorial for sentiment analysis using deep learning, we could use the tokenizer provided by Keras which conveniently created the require mapping(s). Here it is actually easier to to it \"manually\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: (idx + 2) for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {(idx + 2): word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Explicitly set the index of the \"<PAD>\" token\n",
    "word_to_idx['<PAD>'] = 0\n",
    "idx_to_word[0] = '<PAD>'\n",
    "\n",
    "word_to_idx['<UNK>'] = 1\n",
    "idx_to_word[1] = '<UNK>'\n",
    "\n",
    "\n",
    "label_to_idx = {label: (idx + 1) for idx, label in enumerate(labels)}\n",
    "idx_to_label = {(idx + 1): label for idx, label in enumerate(labels)}\n",
    "\n",
    "print(word_to_idx['the'], idx_to_word[4978])\n",
    "print(label_to_idx['I-LOC'], idx_to_label[3])\n",
    "print()\n",
    "print(word_to_idx['<PAD>'], idx_to_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode word and label sequences\n",
    "\n",
    "With the mappings we can now encode the words sequences and pad them so they all have the same length `max_seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = [[word_to_idx[word] for word in sentence] for sentence in X]\n",
    "X_encoded = pad_sequences(X_encoded, maxlen=max_seq_len, padding='post')\n",
    "\n",
    "print(X[0])\n",
    "print(X_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the example above that \"the\" and \"The\" are considered different words. For NER it is often beneficial not to lowercase all words since named entities are often capitalized. In formal text, this assumption typical holds. In informal text like social media, people are typically less careful when it comes to proper capitalization.\n",
    "\n",
    "Encoding the labels is a bit more tricky, since each label needs to be encoded as a one-hot vector instead of a single integer value -- the word indexs will eventually also be converted into word vectors, but this will be done by the `Embedding` layer in the network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function that returns an array of size n, where all elements are zero except at position x, where it is 1.\n",
    "# (i.e., it creates a one-hot encoding for an individual label)\n",
    "def encode(x, n):\n",
    "    result = np.zeros(n)\n",
    "    result[x] = 1\n",
    "    return result\n",
    "\n",
    "# Same as for X_encoded: map label to index and pad to length max_seq_length              \n",
    "y_encoded = [ [label_to_idx[label] for label in label_seq] for label_seq in y ]\n",
    "y_encoded = pad_sequences(y_encoded, maxlen=max_seq_len, padding='post')\n",
    "print(y_encoded[0])\n",
    "print()\n",
    "\n",
    "# Convert each label index to the corresponding one-hot index\n",
    "y_encoded = [ [encode(label, len(labels)+1) for label in label_seq] for label_seq in y_encoded]\n",
    "print(y_encoded[0][:3])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = np.array(X_encoded)\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test data\n",
    "\n",
    "`scitkit-learn` provides a convenient method to split the data into training and test data given a user-defined ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.1)\n",
    "\n",
    "print('Size of training set: {}'.format(len(X_train)))\n",
    "print('Size of test set: {}'.format(len(X_test)))\n",
    "\n",
    "print()\n",
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We have to specify a set of parameters for the network. You can change those value to see the effects on the accuracy and performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_DIM = 32\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "max_features = len(vocabulary) + 2 # +1 because we added the word '<PAD>'; +1 because we added the word '<UNK>'\n",
    "out_size = len(labels) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We define a common network structure containing\n",
    "\n",
    "- an embedding layer\n",
    "- a bi-directional LSTM layer\n",
    "- a fully connected (dense) layer as output for **each time step** (hence `TimeDistributed`)\n",
    "- a softmax layer to normalize the output to probabilities (again, for each time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, EMBEDDING_DIM, input_length=max_seq_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(LSTM_HIDDEN_DIM, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(out_size)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "We can use the in-built method `evaluate()` as usual to calculate the overall accuracy of the model over the test set. However, be aware that we do not simple check of a single class has been predicted currectly. Here, 100% accuracy would now mean that the label for each word in all sequences is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look pretty good. Note that the evaluate can also be application specific. For example, let's take the following sentence \"I stay in the Hilton Hotel\", and the phrase \"The Hilton Hotel\" is labeled as \"I-LOC\" in our dataset. If the our model only label \"Hilton Hotel\" as a location, it would not be 100% correct. One can arguem, however, that this results is correct (enough).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random position of an item in the test data\n",
    "sample_pos = 10\n",
    "\n",
    "# Get the word and label sequence at that position\n",
    "sample_seq = np.asarray([X_test[sample_pos]])\n",
    "sample_labels = np.asarray(y_test[sample_pos])\n",
    "\n",
    "# Convert to a word sequence and get the label\n",
    "word_list = [ idx_to_word[idx] for idx in sample_seq[0] ]\n",
    "label_list = [ np.argmax(label_seq) for label_seq in sample_labels ]\n",
    "\n",
    "print(sample_seq)\n",
    "\n",
    "# Use the trained model to predict the labels for the sample sequence\n",
    "y_pred = model.predict_classes([sample_seq])\n",
    "\n",
    "# Print the word, the predicted label and the true label for each word\n",
    "for i, word in enumerate(word_list):\n",
    "    if word == '<PAD>':\n",
    "        break # If we reach the end of the \"real\" words, we can stop\n",
    "    print(word, idx_to_label[y_pred[0][i]], idx_to_label[label_list[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application use case: extracting named entities from news articles\n",
    "\n",
    "In a previous tutorial we used NLTK and spaCy to extract named entities from news articles. We can now perform this task using our own trained network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "We first need some more packages to fetch news articles and to handle the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility method\n",
    "\n",
    "The following method takes a `sentence` and extracts all found named entities using a traind `model` and the respective `word_to_idx` and `idx_to_label`. The first part convets the sentence into sequence the model understand and predicts the labels; see above example code. The second part - the loop - essentially goes trough the whole sequence to form the output as a list of named entities (phrase + label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(sentence, model, word_to_idx, idx_to_label):\n",
    "    # Tokenize sentence\n",
    "    seq = word_tokenize(sentence)\n",
    "    # Encode words using the word-to-index mapping\n",
    "    seq_encoded = [ word_to_idx[w] if w in word_to_idx else 1 for w in seq ] # Recall: 1 == <UNK>\n",
    "    # Generate final input data by padding the sequence (and truncating if needed!!!)\n",
    "    X = pad_sequences([seq_encoded], maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "    # Use the trained model to predict the labels for the sample sequence\n",
    "    y_pred = model.predict_classes([X], verbose=0)\n",
    "\n",
    "    # Initialize some variables\n",
    "    named_entities, phrase, current_label = [], [], 0\n",
    "    for i, word in enumerate(seq):\n",
    "        if word == '<PAD>':\n",
    "            break # If we reach the end of the \"real\" words, we can stop\n",
    "        # Get predicted label for the current word\n",
    "        label = idx_to_label[y_pred[0][i]]\n",
    "        if label != 'O':\n",
    "            phrase.append(word)\n",
    "        else:\n",
    "            if len(phrase) > 0:\n",
    "                named_entities.append((' '.join(phrase), current_label))\n",
    "            phrase = []\n",
    "        # Update current label\n",
    "        current_label = label\n",
    "        \n",
    "    # Don't forget named entities at the very end even if there is not punctuation mark\n",
    "    if len(phrase) > 0:\n",
    "        named_entities.append((' '.join(phrase), current_label))\n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example regarding the usage of `extract_named_entities`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Tarzan likes to stay at The Hilton Hotel\"\n",
    "\n",
    "named_entities = extract_named_entities(sentence, model, word_to_idx, idx_to_label)\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing news articles\n",
    "\n",
    "Again, we use the `newspaper` package to easily fetch an online news article and extract the main content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears'\n",
    "#url = 'http://www.straitstimes.com/singapore/ammonia-leak-in-food-factory-at-fishery-port-road-3-taken-to-hospital'\n",
    "#url = 'http://www.straitstimes.com/singapore/police-car-mounts-divider-in-accident-in-kampong-bahru-road-no-injuries'\n",
    "\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "title = article.title\n",
    "text = article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained our model of sequences of a maximum length of 64 words. We therefore cannot / should not give the whole content to the model but do it sentence by sentence. To split the content into sentences, we can simply use the `sent_tokenize()` method of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can loop over each sentence and extract all named entities for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    named_entities = named_entities = extract_named_entities(sent, model, word_to_idx, idx_to_label)\n",
    "    print('{}\\n{}\\n\\n'.format(sent, named_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results\n",
    "\n",
    "The results are not bad at all but (of course) far from perfect. There are a couple of reasons for that:\n",
    "\n",
    "- We trained over a very small dataset. 1,500 sentences are arbitrary unlikely to contain enough information and a variety to contain all common language structures that indicate a named entity.\n",
    "- With the small dataset comes also a small vocabulary. As such, the will model often have to deal with unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
