{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation (Deep Learning, LSTM)\n",
    "\n",
    "Machine Translation is a sequence-to-sequence task, where the input is one sequence and the output is another sequences. Sequence-to-sequence tasks have seen a great improvement with the advent of Deep Learning. Various network models have been proposed and are still proposed for further improvements. In this tutorial, we rely on recurrent network models (in more detail: LSTMs). The connection between the two models can be sketched as follows:\n",
    "\n",
    "<img src=\"images/rnn-many-to-many-machine-translation.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "The left LSTM network, the encoder, reads the input sequence. The hidden states of the LSTM cell is the copied to the decoder (righthand-side network), which uses this state to generate the new sequence.\n",
    "\n",
    "**Task:** In this tutorial we translate keyword-based queries into the corresponding questions in proper English. Each query is about a restaurant or a hotel, and we assume that each restaurant or hotel name has already been exracted using NER and replaced with `<POI>` (point of interest). This is a common step for two reasons:\n",
    "\n",
    "* Named entities often contain words that are not in the dictionary. Excluding them makes the vocabulary significantly smaller, thus easier for the network to train.\n",
    "* Named entities -- here the names of restaurant and hotels -- are usually not translated anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, TimeDistributed, Activation, Embedding, Dense, Bidirectional, RepeatVector, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from utils.timeutil import convert_seconds_to_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset file\n",
    "\n",
    "If you look into the text file `data/seq2seq-dataset/closeup-dataset-q2q-1m.txt` you can see that each line contains a pair of strings seperated by a tabulator `\\t`. The first string represents a keyword-based query while the seconds string id the corresponding question in proper English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "with open('data/seq2seq-dataset/closeup-dataset-q2q-1m.txt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        query, question = line.strip().split('\\t')\n",
    "        pairs.append([query, question])\n",
    "        \n",
    "# Convert list to numpy array for convenience\n",
    "pairs = np.array(pairs)\n",
    "        \n",
    "for i in range(10):\n",
    "    #print(pairs[i])\n",
    "    print('[%s] => [%s]' % (pairs[i,0], pairs[i,1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and test data\n",
    "\n",
    "We first limit the number of query-question pairs to 10,000. While this is not enough to get proper results, the training is reasonable fast. Only when everything works fine, one can increase the size of the dataset for training.\n",
    "\n",
    "We take 99% of all query-question pairs for the training since we only need some test data for mannually inspecting the results. Reliably quantifying the accuracy of machine translation is still an open challenge; some commonly used measured do exist, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = pairs[:n_sentences, :]\n",
    "\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "\n",
    "# split into train/test\n",
    "train_ratio = 0.99\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "train, test = dataset[:train_size], dataset[train_size:]\n",
    "\n",
    "print('Size of training data: {}'.format(len(train)))\n",
    "print('Size of test data: {}'.format(len(test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data enconding\n",
    "\n",
    "As usual, we need to properly encode the data to be used as input and output for the deep learning network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This little auxiliary method returns the longest string (in terms int the number of words/tokens) in a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use the `Tokenizer` class provided by Keras to generate the vocabulary and the word-to-index mapping. Since we do machine translation, we need to do this for both the input sequences (queries) and the output sequences (questions). Strictly speaking, since input and output sequences are both in English, one tokenizer would suffice in this case. However, we pretent that sequences are in different \"languages\" to make it more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_tokenizer = Tokenizer(filters='', lower=False)\n",
    "query_tokenizer.fit_on_texts(dataset[:, 0])\n",
    "query_vocab_size = len(query_tokenizer.word_index) + 1\n",
    "query_length = max_length(dataset[:, 0])\n",
    "print('Query vocabulary Size: %d' % query_vocab_size)\n",
    "print('Query max length: %d' % (query_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokenizer = Tokenizer(filters='', lower=False)\n",
    "question_tokenizer.fit_on_texts(dataset[:, 1])\n",
    "question_vocab_size = len(question_tokenizer.word_index) + 1\n",
    "question_length = max_length(dataset[:, 1])\n",
    "print('Question vocabulary Size: %d' % question_vocab_size)\n",
    "print('Question max length: %d' % (question_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followinf to auxiliary methods do the actual encoding of the data to enable the training:\n",
    "    \n",
    "* `encode_sequences()`: the method first converts the sequences from strings into list of word indexes and then pads each list with zeros so that all lists have the same length\n",
    "\n",
    "* `encode_output()`: only needed for the output sequences (i.e., the questions); in encodes each list of word indexes into a list of one-hot vectors of size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines just illustrate how the encoded input and the encoded output looks like for the first query-question pair in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sample_encoded = encode_sequences(query_tokenizer, query_length, train[0:1, 0])[0]\n",
    "\n",
    "question_sample_encoded = encode_sequences(question_tokenizer, question_length, train[0:1, 0])\n",
    "question_sample_encoded = encode_output(question_sample_encoded, question_vocab_size)[0]\n",
    "\n",
    "print(query_sample_encoded.shape)\n",
    "print(query_sample_encoded)\n",
    "print()\n",
    "print(question_sample_encoded.shape)\n",
    "print(question_sample_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "* The `Embedding` layers vectorizes the input sequences; each input sequences is list of word indexes\n",
    "\n",
    "* The first `LSTM` layer represent the encoder\n",
    "\n",
    "* `RepeatVector` facilitiates the copying of the hidden layers of the encoder to the hidden layer of the decoder\n",
    "\n",
    "* The second `LSTM` layer represents the decoder; since the output is a sequence and not just one word, we need to return all vectors of the hidden layer (i.e., the vector of the hidden layer at each time step)\n",
    "\n",
    "* The last layer is fully connected (`Dense`); notice that it is wrapped in a `TimeDistributed` layer, again since we need multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(query_vocab_size, n_units, input_length=query_length, mask_zero=True))\n",
    "model.add(LSTM(n_units))\n",
    "model.add(RepeatVector(question_length))\n",
    "model.add(LSTM(n_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(question_vocab_size, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "In principle, we can simply call `model.fit()` to train the network; see previous tutorials. But here we have a little problem: Encoding the input and input sequences will require too much memory and result in an error. Remember the output sequences have a shape of (#items, #max_length, #vocabulary_size). For example, for 100,000 datapoints, a maximum sequence length of 15 and a vocbulary size of 1,000 words, would require to store 1,500,000,000 numbers in the main memory. Even with the internal optimization this requirement is too much for a commodity PC.\n",
    "\n",
    "To address this, we have to train the network in blocks, e.g., only 1,000 query-question pairs at a time. This includes that we have to manually implement the notion of epochs.\n",
    "\n",
    "Let's first define a auxiliary method than encodes the input and output sequences for a current block of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(macro_batch, num_samples):\n",
    "    start = macro_batch * num_samples\n",
    "    end = (macro_batch+1) * num_samples\n",
    "    # prepare training data\n",
    "    X_train = encode_sequences(query_tokenizer, query_length, train[start:end, 0])\n",
    "    y_train = encode_sequences(question_tokenizer, question_length, train[start:end, 1])\n",
    "    y_train = encode_output(y_train, question_vocab_size)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "The training is now 2 nested loops. The outer loop handles the number of epochs (e.g., 10). The inner loop generates the next block of training data (e.g., 1,000 query-question pairs) and trains the network with that block. The other commands just measure the execution time and print some informative output.\n",
    "\n",
    "Note that in `model.fit()` we set `epochs=1` since we handle the number of epochs now in the outer loop ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "num_macro_batches = int(np.ceil(len(train) / NUM_SAMPLES))\n",
    "\n",
    "filename = 'data/trained-models/q2q.keras'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "start = timer()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"=====================================================\")\n",
    "    print(\"MACRO EPOCH: {}\".format(epoch+1))\n",
    "    start_epoch = timer()\n",
    "    for macro_batch in range(num_macro_batches):\n",
    "        X_train, y_train = prepare_data(macro_batch, NUM_SAMPLES)\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=64, callbacks=[checkpoint], verbose=0, validation_split=0.1)\n",
    "    end_epoch = timer()\n",
    "    \n",
    "    execution_time_epoch = convert_seconds_to_time(end_epoch - start_epoch)\n",
    "    execution_time = convert_seconds_to_time(end_epoch - start)\n",
    "    \n",
    "    print(\">>> epoch execution time {}, overall execution time: {}\".format(execution_time_epoch, execution_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice, even with just around 10,000 input pairs, the training now will quite take some time. If you use this trained model, you will also notice that rather bad results. In practice, you need to train the network with much more data and more epochs.\n",
    "\n",
    "To see the improvement, you can load the weights that have been trained using about 1 million data pairs over 10 epoch. the training required about 15-20 hours on a modern commodity PC (CPU only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#model.load_weights('data/trained-models/q2q.keras')\n",
    "#model.load_weights('data/trained-models/q2q-1m-10epochs.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In this tutorial we evaluate the network only by inspecting individual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following auxiliary methods simply converts a word index back into the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `predict_sequence()` predicts the output sequence (question) for a single input sequence (query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a couple of queries from the set of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample = encode_sequences(query_tokenizer, query_length, test[0:10, 0])\n",
    "\n",
    "print(X_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, source in enumerate(X_test_sample):\n",
    "    # translate encoded source text\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "    translation = predict_sequence(model, question_tokenizer, source)\n",
    "    raw_src, raw_target = test[i]\n",
    "    print('========================================')\n",
    "    print('input query: %s\\ntrue question: %s\\npredicted question: %s' % (raw_src, raw_target, translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
