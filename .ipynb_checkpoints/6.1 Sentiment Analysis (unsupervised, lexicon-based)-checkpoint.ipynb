{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (unsupervised, lexicon-based)\n",
    "\n",
    "With the main preprocessing steps in place, we can start with higher-level text analysis. In this tutorial, we perform sentiment analysis using so called sentiment lexicons. A sentiment lexicon is a set of words that have be labeled with \"positive\" or \"negative\" or a numerical value reflecting the sentiment of the word. For example, the word \"happy\" is intuitively associated with a positive sentiment.\n",
    "\n",
    "This approach is called unsupervised since it does not rely on the labeld input data -- i.e., the text for which we want to calculate the sentiment does not need to be labeled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# The next imports are only needed for the preprocessing\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.nlputil import preprocess_text\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "\n",
    "We need to load and prepare two types of data:\n",
    "\n",
    "* the sentiment lexicon\n",
    "\n",
    "* the input data (here: tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process sentiment lexicon\n",
    "\n",
    "We use `pandas` to load the publicly availble VADER sentiment lexicon. The advantage of this lexicon compared to others is that it also contains a wide range of non-word such as emoticons. This is useful when dealing with social media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('data/sentiment-lexicon/sentilex-vader.txt', sep='\\t', encoding = \"ISO-8859-1\", header=None)\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the lexicon contains the 10 individual labels from each annotator, we only need the average value in Column 1. While not crucial here, we also normalized the sentiment scores from [-4, ..., 4] to [-1, ..., 1]. This is usually a good practice when combining different sentiment lexicons since many use a different scoring theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {}\n",
    "\n",
    "for index, row in df_sentiment.iterrows():\n",
    "    token, score = row[0], row[1]\n",
    "    sentiment_dict[token] = score / 4.0 # normalize score from [-4,...,4] to [-1,...,1]\n",
    "\n",
    "# Print a random sample for     \n",
    "print (random.sample( sentiment_dict.items(), 5 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate the approach with 2 example sentences, one positive and one negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = \"I was very happy with the service.\"\n",
    "neg = \"I wasn't very happy with the service.\"\n",
    "\n",
    "documents = [pos, neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we first need to preprocess our input documents. Most sentiment lexicons only contain words in the base form, e.g., \"happy\", but not derived forms, e.g., \"happier\". Not also that not only adjectives are associated with a sentiment but also nouns such as \"love\", \"hate\", \"mistake\", \"luck\", etc. as well as verbs such as \"celebrate\", \"suffer\", \"enjoy\", etc.\n",
    "\n",
    "Note that we do NOT remove stopwords, since \"not\" and \"n't\" are considered a stopworda and removing them would clearly alter the meaning of a document. You can try removing the stopwords and see it's effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents = [''] * len(documents)\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    #processed_reviews[idx] = preprocess_text(doc, remove_stopwords=False, remove_punctuation=False)\n",
    "    #processed_reviews[idx] = preprocess_text(doc, stemmer=porter_stemmer, remove_stopwords=False, remove_punctuation=False)\n",
    "    processed_documents[idx] = preprocess_text(doc, lemmatizer=wordnet_lemmatizer, remove_stopwords=False, remove_punctuation=False)\n",
    "    \n",
    "print (processed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through both documents check if a word is in the sentiment document. If so, we add the sentiment score of the word to the overall score `review_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    review_score = 0.0\n",
    "    for token in doc.split(): # Here split() is sufficient\n",
    "        if token in sentiment_dict:\n",
    "            review_score = review_score + sentiment_dict[token]\n",
    "    print (review_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both documents got the same score although they clearly express opposite sentiments. That's because we didn't properly handle the negation. \"not\" or \"n't\" themselves are not association with any sentiment score, but indicated that the scores of the following words need to be flipped (change of sign).\n",
    "\n",
    "To accomplish this, we first need to know which words (beyond \"not\" and \"n't\") can flip the polarities of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negation_words = pd.read_csv('data/word-lists/english-negation-words-lowercase.txt', sep='\\t', encoding = \"ISO-8859-1\", header=None)\n",
    "\n",
    "negation_words = df_negation_words[0].tolist()\n",
    "\n",
    "print(negation_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now improve the scoring method. To what extent a negation word is effecting a document is actually a challenging task. In the following, we adopt the common and simple heuristic that we flip the sentiment score of the next 3 words -- that is, the scope of a negation is the list if the 3 succeeeding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in processed_documents:\n",
    "    review_score = 0.0\n",
    "    negation_scope = -1\n",
    "    \n",
    "    for token in doc.split(): # Here split() is sufficient\n",
    "        if token in negation_words:\n",
    "            negation_scope = 2\n",
    "        if token in sentiment_dict:\n",
    "            token_score =  sentiment_dict[token]\n",
    "            if negation_scope >= 0: # If we are still in the negation scope, flip the sentiment score\n",
    "                token_score *= (-1)\n",
    "            review_score = token_score\n",
    "        negation_scope -= 1 # Reduce the negation score by 1\n",
    "    print (review_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we handle the negation in a better way. Be aware, however, that this approach is far from fool-proof. For example, image the 2nd sentence is *\"I wasn't really very very happy with the service.\"*. Here, the word \"happy\" would be outside of the scope of the negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process Twitter data\n",
    "\n",
    "We use a publicly dataset of several hundreds of tweets. All tweets a labeled with a polarity:\n",
    "\n",
    "* 0 = negative\n",
    "* 2 = neutral\n",
    "* 4 = negative\n",
    "\n",
    "Note that we need these labels only to evalute the performance and not for any training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-training.csv')\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the polarities and tweets in 2 separate lists for further processing. The list `polarities` contains to true polarities of each tweet we can use to evaluate our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities = df_tweets['senti'].tolist() \n",
    "tweets = df_tweets['tweet'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to properly preprocess all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = [''] * len(tweets)\n",
    "\n",
    "for idx, doc in enumerate(tweets):\n",
    "    #processed_tweets[idx] = preprocess_text(doc, remove_stopwords=False, remove_punctuation=False)\n",
    "    #processed_tweets[idx] = preprocess_text(doc, stemmer=porter_stemmer, remove_stopwords=False, remove_punctuation=False)\n",
    "    processed_tweets[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer, remove_stopwords=False, remove_punctuation=False)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment calculation\n",
    "\n",
    "The following two methods do the same steps as shown above. Both loop over all tweets and check for all words in each tweet if a word has a sentiment score (i.e., is in the sentiment lexicon). Depending on the overall score for a tweet (less than 0 or greater than 0) it assigns the respective polarity to the tweet (0, 2, or 4). The method `calc_polarities_negation()` considers the scope of a negation the same way as shown above: flip the sentiment scores of the next 3 words.\n",
    "\n",
    "Note that there is one special case. Let's assume we only want to label the tweets with \"positive\" or \"negative\" (thus, only 2 classes). How do we handle tweets with an overall score of 0. In this case, we assume that the tweet is positive. \n",
    "\n",
    "Both methods return a list of polarity labels, e.g., `[2, 0, 0, 2, 0, 4, 2, 0, 4, 4, ...]`. The length of the list is the number of documents (tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_polarities(docs, num_polarities=3):\n",
    "    calculated_polarities = []\n",
    "    for doc in docs:\n",
    "        doc_score = 0.0\n",
    "        for token in doc.split(): # Here split() is sufficient\n",
    "            if token in sentiment_dict:\n",
    "                doc_score += sentiment_dict[token]\n",
    "        if doc_score > 0:\n",
    "            calculated_polarities.append(4)\n",
    "        elif doc_score < 0:\n",
    "            calculated_polarities.append(0)\n",
    "        else:\n",
    "            if num_polarities == 3:\n",
    "                calculated_polarities.append(2)\n",
    "            else:\n",
    "                calculated_polarities.append(4) # By default, assume it's positive (for 2 classes)\n",
    "    return calculated_polarities\n",
    "\n",
    "\n",
    "def calc_polarities_negation(docs, num_polarities=3):\n",
    "    calculated_polarities = []\n",
    "    for doc in docs:\n",
    "        doc_score = 0.0\n",
    "        negation_scope = -1\n",
    "        for token in doc.split(): # Here split() is sufficient\n",
    "            if token in negation_words:\n",
    "                negation_scope = 2\n",
    "            if token in sentiment_dict:\n",
    "                token_score = sentiment_dict[token]\n",
    "                if negation_scope >= 0:\n",
    "                    token_score *= (-1)\n",
    "                doc_score += token_score\n",
    "            negation_scope -= 1\n",
    "        if doc_score > 0:\n",
    "            calculated_polarities.append(4)\n",
    "        elif doc_score < 0:\n",
    "            calculated_polarities.append(0)\n",
    "        else:\n",
    "            if num_polarities == 3:\n",
    "                calculated_polarities.append(2)\n",
    "            else:\n",
    "                calculated_polarities.append(4) # By default, assume it's positive (for 2 classes)\n",
    "    return calculated_polarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the polarities for all tweets using the methods as defined above. You can try both methods (with or without negation handling) and see its effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated_polarities = calc_polarities(processed_tweets)\n",
    "calculated_polarities = calc_polarities_negation(processed_tweets)\n",
    "\n",
    "print(calculated_polarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Common metrics\n",
    "\n",
    "We can now use existing metrics to evaluate how well our sentiment analysis performed. Recall that `polarities` contains the true polaritie labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (confusion_matrix(polarities, calculated_polarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(polarities, calculated_polarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combined f1-score is actually not bad value for sentiment analysis. Sentiments are often highly subjective, and often annotator disagree significantly when labeling a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis\n",
    "\n",
    "A common task part of the evaluation is the so-called error analysis. The idea is to manually inspect all misclassified documents to get an idea in what cases the sentiment assignment fails. This in turn serves as basis to improve the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wrong_predictions = 0\n",
    "\n",
    "for idx in range(len(polarities)):\n",
    "    true_polarity = polarities[idx]\n",
    "    pred_polarity = calculated_polarities[idx]\n",
    "    if true_polarity != pred_polarity:\n",
    "        num_wrong_predictions += 1\n",
    "        print (\"True: {}, Predicted: {}, Tweet: {}\".format(true_polarity, pred_polarity, tweets[idx]))\n",
    "        \n",
    "print (num_wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
