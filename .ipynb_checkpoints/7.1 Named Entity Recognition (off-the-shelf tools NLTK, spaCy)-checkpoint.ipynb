{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (off-the-shelf tools NLTK, spaCy)\n",
    "\n",
    "Named Entity Recognition has become such a common task that many available tools provide NER out of the box. In this tutorial, we use NLTK and spaCy to perform this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk import sent_tokenize, sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the English model for spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a document\n",
    "\n",
    "For this tutorial, we use news articles as documents using the `newspaper` package to make it easy for us; see the \"Data Collection\" tutorial for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears'\n",
    "url = 'http://www.straitstimes.com/singapore/ammonia-leak-in-food-factory-at-fishery-port-road-3-taken-to-hospital'\n",
    "#url = 'http://www.straitstimes.com/singapore/police-car-mounts-divider-in-accident-in-kampong-bahru-road-no-injuries'\n",
    "\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods `download()` and `parse()` fetch and process the current article. For the rest of this tutorial, we only consider the title and the main content (text) of an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "title = article.title\n",
    "text = article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with NLTK\n",
    "\n",
    "We first show how NLTK performs NLTK with a simple example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Straits Times interviewed PM Lee last week in Japan.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocessing we need to tokenize and POS-tag the sentence, since POS tags a required for the NER extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence\n",
    "token_list = word_tokenize(example_sentence)\n",
    "# POS tag token list\n",
    "token_pos_list = pos_tag(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK proivdes a method `ne_chunk` (ne = named entity) which labels words or phrases as named entities. The result is a token tree, an internal representation used in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform named entity recogniztion through chunking\n",
    "ne_chunk_tree = ne_chunk(token_pos_list, binary=False)\n",
    "\n",
    "print(ne_chunk_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following auxiliary method goes trough the tree and extract the named entities and put them with their label into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(tree):\n",
    "    chunk_list = []\n",
    "    for i in tree:\n",
    "        if type(i) == Tree:\n",
    "            label = i.label()\n",
    "            name = \" \".join([token for token, pos in i.leaves()])\n",
    "            chunk_list.append((name, label))\n",
    "        else:\n",
    "            continue\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_named_entities(ne_chunk_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repate this steps with the news article. First we need to tokenize the document into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, we perform the required steps as shown above:\n",
    "\n",
    "* tokenize sentice into words/tokens\n",
    "* POS-tag token list\n",
    "* Perform NER using `ne_chunk()`\n",
    "* Extract found named entities and put them into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    # Tokenize sentence\n",
    "    token_list = word_tokenize(sent)\n",
    "    # POS tag token list\n",
    "    token_pos_list = pos_tag(token_list)\n",
    "    # Perform named entity recogniztion through chunking\n",
    "    ne_chunk_tree = ne_chunk(token_pos_list, binary=False)\n",
    "    # Extract the named entities from tree into a simple list\n",
    "    named_entities_list = extract_named_entities(ne_chunk_tree)\n",
    "    # Print found named entities\n",
    "    for ne in named_entities_list:\n",
    "        print(\"-- {} ({})\".format(ne[0], ne[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When spaCy analyzes a document, it performs a whole series steps including tokenizing, POS tagging, lemmatization and even named entity recognition. That means, after performing the following command, the `doc` object already contains the intormation about found named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auxiliary method `show_named_entities()` simply prints all found named entities in a nice layout. The only additional - an optional - parameter is `valid_labels` that allows to specify a list of named entity types. For example, we can simply print all persons by setting `valid_labels=['person']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_named_entities(doc, valid_labels=None):\n",
    "    print('{:35} {:25} {:6} {:6}'.format(\"NAME\", \"LABEL\", \"START\", \"END\"))\n",
    "    for e in doc.ents:\n",
    "        label = e.label_.lower()\n",
    "        if valid_labels is not None:\n",
    "            if label not in valid_labels:\n",
    "                continue\n",
    "        # Print name, label, start and end (in a nice way)\n",
    "        print('{:35} {:25} {:5} {:5}'.format(e.text, label, e.start_char, e.end_char))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the found named entities for the current document. You can filter the list by specifying a list of valid labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_named_entities(doc)\n",
    "#show_named_entities(doc, valid_labels=['gpe', 'loc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of entitiy types supported by spaCy\n",
    "\n",
    "![title](images/ner-spacy-entity-labels.png \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
