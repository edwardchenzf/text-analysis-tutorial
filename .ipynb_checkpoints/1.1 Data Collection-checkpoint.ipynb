{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This tutorials just give some examples how to fetch and handle different types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_name = 'data/sample-text-files/sample-text-file-1000kb.txt'\n",
    "\n",
    "documents = []\n",
    "with open(sample_file_name) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line != '': # Ingnore empty lines\n",
    "            documents.append(line)\n",
    "            \n",
    "print(\"The file {} contains {} documents.\".format(sample_file_name, len(documents)))\n",
    "print()\n",
    "print(\"This is the last document:\")\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV/TSV files\n",
    "\n",
    "In principle, CSV/TSV (comma-separated/tab-separated values) files are also just text files. As such, one can sue the approache from above to read such files. The structured nature of CSV/TSV files quickly leads to annoying issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_file_name = 'data/reviews/yelp-reviews-mon-ami-gabi.csv'\n",
    "\n",
    "with open(reviews_file_name) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0: # We want to ignore the header\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            review_nr, review_text = line.split(',') # Oh, oh...can you spot the problem?\n",
    "            print(review_text) # This will most likely throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since handling CSV/TSV files is a very common task, there is already powerful Python packages available that makes life some much easier. `pandas` is a very popular package for handling structured files like CSV/TSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` uses the notion of data frames (df) to denote data objects\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(reviews_file_name, sep=',', quotechar='\"', encoding = \"ISO-8859-1\")\n",
    "df = pd.read_csv(reviews_file_name, encoding = \"ISO-8859-1\")\n",
    "\n",
    "df.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of reviews from data frame\n",
    "reviews = df['review'].tolist()\n",
    "\n",
    "print(\"The file {} contains {} reviews.\".format(reviews_file_name, len(reviews)))\n",
    "print()\n",
    "print(\"This is the last review:\")\n",
    "print(reviews[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online news article\n",
    "\n",
    "This example addresses online content. Handling \"raw\" websites is usually a bit annoying since the text is not plain text but HTML. For simplicity, we use the package `newspaper` that helps to fetch the content of online news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to copy&paste different news article URLs. Note the package does not work with all news websites; however, it works just fine with straitstimes.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears'\n",
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods `download()` and `parse()` do the actually fetching and processing of the news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Authors:\", article.authors, \"\\n\")\n",
    "print(\"Publication data:\", article.publish_date, \"\\n\")\n",
    "print(\"Title:\", article.title, \"\\n\")\n",
    "print(\"Main text:\", article.text, \"\\n\")\n",
    "print(\"Top image link:\", article.top_image, \"\\n\")\n",
    "print(\"Video links:\", article.movies, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `newspaper` packages comes with some additional functions to extract keywords and generate a summary for a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.nlp()\n",
    "print(\"Keywords:\", article.keywords, \"\\n\")\n",
    "print(\"Summary:\", article.summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter provides an API (Application Programming Interface) that allows to fetch public tweets. The `twython` packages is a wrapper for ths API to simplify this task in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython, TwythonError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the API requires credentials. This in turn requires a Twitter account and further configurations. If you don't have or want an Twitter account then no problem. This tutorial is only supposed to show how simply the task of fetching tweets is. It won't be required for the other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_KEY = '' \n",
    "APP_SECRET = '' \n",
    "OAUTH_TOKEN = '' \n",
    "OAUTH_TOKEN_SECRET = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other calls, the Twitter API allows to search for tweets using keywords. You can also specify that you're not interest in retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_results = twitter.search(q='\"orchard road\" -filter:retweets', count=20)\n",
    "except TwythonError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet comes with a plethora of information. In the following we are only interest in the user name, the date and time the tweet was posted and the tweet text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in search_results['statuses']:\n",
    "    # Ingnore non-English tweets\n",
    "    language = tweet['lang']\n",
    "    if language != 'en':\n",
    "        continue\n",
    "    # Extract the basic information about the tweet\n",
    "    screen_name = tweet['user']['screen_name']\n",
    "    created_at =  tweet['created_at']\n",
    "    tweet_text = tweet['text']    \n",
    "    # Simple way to remove line breaks and tabs: string to list and back to string again\n",
    "    tweet_text = ' '.join(tweet_text.split())\n",
    "    # Twitter returns the time as string of the form \"Wed Jan 24 10:37:57 +0000 2018\"; let's simplify this\n",
    "    created_at = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(created_at,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "    # Print each tweet with publication date, the screen name of the user, and the actual text of the tweet\n",
    "    print('[{}] @{} wrote: {}'.format(created_at, screen_name, tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
