{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (Deep Learning, MLP)\n",
    "\n",
    "In this tutorial, we perform sentiment analysis using deep learning, where we use a basic Multilayer Perceptron (MLP) network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The next imports are only needed for the preprocessing\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.nlputil import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a tokenizer and Lemmatizer for the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "\n",
    "We use `pandas` as usual to read the file with the tweets and assigned sentiment labels. In this case, we have 2 files, one containing the traing data and the test data. For both files we perform the following steps:\n",
    "\n",
    "* Store tweets and labels (polarities) into separate lists for further processing\n",
    "* Preprocess the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_train = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-training.csv')\n",
    "\n",
    "# Print the first 5 lines\n",
    "df_tweets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = df_tweets_train['tweet']\n",
    "train_polarities = df_tweets_train['senti']\n",
    "\n",
    "train_tweets_processed = [''] * len(train_tweets)\n",
    "for idx, doc in enumerate(train_tweets):\n",
    "    train_tweets_processed[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_test = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-test.csv')\n",
    "\n",
    "test_tweets = df_tweets_test['tweet']\n",
    "test_polarities = df_tweets_test['senti']  \n",
    "\n",
    "test_tweets_processed = [''] * len(test_tweets)\n",
    "for idx, doc in enumerate(test_tweets):\n",
    "    test_tweets_processed[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training data: {}\".format(len(train_tweets_processed)))\n",
    "print(\"Number of test data: {}\".format(len(test_tweets_processed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode / vectorize data\n",
    "\n",
    "We need to encode both the tweets as well as the labels to use them as input and output for the network.\n",
    "\n",
    "For convenience, we use the `Tokenizer` class provided by Keras. It generates a document term matrix with binaty weights. We can limit the vocabulary the *N* most frequently used words (here *N=250*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 250\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_tweets_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = tokenize.texts_to_matrix(train_tweets)\n",
    "X_train = tokenizer.texts_to_matrix(train_tweets_processed)\n",
    "\n",
    "#X_test = tokenize.texts_to_matrix(test_tweets)\n",
    "X_test = tokenizer.texts_to_matrix(test_tweets_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, print the vector for the first tweet. The vector has a length of 250 (the size of the vocabulary). A 1 at position *i* indicates that the tweet contains the word with the word index *i*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 3 classes (positive, netural, negative) our network will have a output layer of 3 neurons. For the network to calculate the costs given a predicted output (3 values) and the true output, we need to convert the true label to vector with also 3 values. \n",
    "\n",
    "We can use existing methods to easily convert the true labels into the respective one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_polarities)\n",
    "y_train = encoder.transform(train_polarities)\n",
    "y_test = encoder.transform(test_polarities)\n",
    "\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network model\n",
    "\n",
    "We are use MLP, i.e., a simple stack of fully connected layers (`Dense`). Apart from the input layer, we also define\n",
    "\n",
    "* 1st hidden layer of size 512\n",
    "* 2nd hidden layer of size 256\n",
    "* Output layer of size 3 (on for each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3 # We have 3 polarity classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(256)) # No need to specify input size - is derived from output of previous layer\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MLPs it's easy to calculate the number of parameters manually, so let's to this for illustrative purposes:\n",
    "\n",
    "* 1st hidden layer: 250 (input layer size) * 512 (1st hidden layer size) + 512 (1st hidden layer size; for biases) = 128,512\n",
    "\n",
    "* 2nd hidden layer: 512 (input layer size) * 256 (2nd hidden layer size) + 256 (2nd hidden layer size; for biases) = 131,328\n",
    "\n",
    "* Output layer: 256 (2nd hidden layer size) * 3 (output layer size) + 3 (output layer size, for biases) = 771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Compiling the model essentially initializes all the weights with some random value. We also specify here which loss and which optimizer we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "With everyhing in place, we can train the model by calling the `fit()` method. Apart from the training data and labels, the method takes the followning input parameters:\n",
    "\n",
    "* `batch_size`: the number of training data that are evaluated in one pass\n",
    "\n",
    "* `epochs`: the number of times the whole training data is passed through the network\n",
    "\n",
    "* `verbose`: different levels of outputs to follow the training progress\n",
    "\n",
    "* `validation_split`: ratio of how much of the data is used for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is rather fast simply because we only use ~700 tweets for the training which is a very small dataset for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "We already prepared the test data. We therefore can use it as input for the provided method `evaluate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More detailed evaluation\n",
    "\n",
    "Keras does not come with any more detailed information regarding an evaluation beyond the accuarcy value. However, one can simple use the methods that come with `scikit-learn` for that; see previous tutorials.\n",
    "\n",
    "We first predict the classes/polarities for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes([X_test])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` contains the values 0, 1 and 2, representing the three classes *negative*, *neutral* and *positive*, respectively. However, the original labels are 0 (*negative*), 2 (*neutral*) and 4 (*positive*). Therefore need to normalize the label so that they match. In our this this is easy, we just need to divide the original labels by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_normalized = [ int(p/2) for p in test_polarities ] # Works as well\n",
    "y_test_normalized = np.asarray([ int(p/2) for p in test_polarities ])\n",
    "print(y_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the useful function provdided by `scikit-learn` to print more detailed values such as precision, recall and f1-score for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_normalized, y_pred))\n",
    "print()\n",
    "print(classification_report(y_test_normalized, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
